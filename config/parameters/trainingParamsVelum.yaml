
### ~\config\parameters\configTraining.yaml
targetColName: percentage
epochs : 150
dataLoaderArgs :  {'batch_size': 1, 'num_workers': 4,'drop_last': True}
normalize : False
scheduler: plateau
makeTest: True
hydra: default
override hydra/hydra_logging: colorlog # enable color logging to make it pretty
override hydra/job_logging: colorlog # enable color logging to make it pretty
modelMode: train
# @package _global_
##  Model
model:
  _target_: model_set.models.UNetClassiFlood
  classes : 1
  in_channels : 1
  dropout: True   # (bool) Use dropout or not
  prob: 0.1 # float: 0. - 0.99
  # Model options
  #  UNetClassiFlood: (For this moldel uou can add ->> classifierOn : True)
negative_slope_linear : 0.0001 # To apply to LeakyRelu
negative_slope_Encoder : 0.0001 # To apply to the encoder block. 0.01 is the default value of the function: nn.LeakyReLU() 
init_weight:
  _partial_: True 
  _target_: torch.nn.init.kaiming_normal_
initWeightParams: {mode:'fan_out', nonlinearity :'leaky_relu'}
## Loss
loss_fn:
  _partial_: True 
  _target_: torch.nn.functional.binary_cross_entropy_with_logits
  # Loss Options:
  #  torch.nn.functional.binary_cross_entropy_with_logits
  #  torch.nn.BCELoss
  #  scr.losses.lovasz_hinge (Note: To be used with Optimizer/maximize : True)
## Metric
metric_fn :
  _partial_: True
  _target_: scr.losses.iou_binary 
## Optimizer
optimizer:
  _target_: torch.optim.Adam
  maximize : False
  lr: 0.001
  weight_decay : 0.5