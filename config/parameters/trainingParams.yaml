
### ~\config\parameters\configTraining.yaml
targetColName: percentage
epochs : 50
dataLoaderArgs :  {'batch_size': 1, 'num_workers': 4,'drop_last': True}
normalize : False
inlineTransform : False
scheduler: plateau
makeTest: True
hydra: default
override hydra/hydra_logging: colorlog # enable color logging to make it pretty
override hydra/job_logging: colorlog # enable color logging to make it pretty
# @package _global_
##  Model
model:
  _target_: model_set.models.UNetClassiFlood
  classes : 1
  in_channels : 1
  dropout: True   # (bool) Use dropout or not
  prob: 0.01  # float: 0. - 0.99
  addParams : {patch_W: 384,patch_H: 384, negative_slope_linear: 0.01, negative_slope_Encoder : 0.01}  # Only for UnetClassiFlood
  #classifierOn : False    # Only for UnetFlood
init_weight:
  _partial_: True 
  _target_: torch.nn.init.normal_
  # torch.nn.init.kaiming_normal_  -->> initWeightParams: {mode:'fan_out', nonlinearity :'leaky_relu'} #Options : nonlinearity :'leaky_relu' or 'relu' 
  # torch.nn.init.normal_  -->> initWeightParams:{mean:0.0, std : 0.01}
initWeightParams: {mean:0.0, std : 0.01} 
## Loss
loss_fn:
  _partial_: True 
  _target_: scr.losses.lovasz_hinge  #torch.nn.functional.binary_cross_entropy_with_logits  #
## Metric
metric_fn :
  _partial_: True 
  _target_: scr.losses.iou_binary   #torcheval.metrics.functional.binary_accuracy  
## Optimizer
optimizer:
  _target_: torch.optim.Adam
  maximize : True
  lr: 5
  #weight_decay : 0.1